\chapter{Benchmark and Analysis}  \label{chap:bench_analysis}

\begin{chapquote}{Michael A. Jackson}
	Rules of Optimization:
	Rule 1: Don't do it.
	Rule 2 (for experts only): Don't do it yet.
\end{chapquote}

\ref{chap_impl:sec:balances} introduced an example runtime module in our implementation, namely a
balances module that can store the balance of different accounts and initiate transfers between
them. In this chapter we build upon this module and provide benchmarks to evaluate SonicChain, as
described in \ref{chap_desgin:sec:our_approach}. First, we begin by explaining the details of the
benchmarking environment, including the data set.

\section{The Benchmarking Environment}

All experiments are executed on a personal laptop with \texttt{2,3 GHz 8-Core Intel Core i9 CPU} and
\texttt{32 GB 2400 MHz DDR4 RAM}. We keep the machine connected to power for consistent results, and
run no additional resource-intensive software while taking measurements.

We measure the execution time of both the \textit{authoring} and \textit{validation} tasks. From the
computed time, we derive the throughput in \textit{transactions per second}. Recall that authoring
is the process of creating a block, and validating is the task of re-importing it to ensure
veracity; these tasks are performed by the \textit{author} and \textit{validator}, respectively.
Moreover, recall that in our concurrency delegation model, by the end of the authoring phase, all
transactions are tagged with the identifier of the thread that should execute them. Therefore, the
validation task is fairly simpler. Furthermore, we set the \texttt{access} macro of the transfer
transaction to point to the account balance key of the \texttt{origin} and \texttt{destination}
account, as demonstrated in listing \ref{lst:balance_sig}.

\begin{lstlisting}[caption={Signature of the Transfer and its Access Hints}\label{lst:balance_sig}]
#[access = (|origin| vec![
	<BalanceOf<R>>::key_for(origin),
	<BalanceOf<R>>::key_for(dest)
])]
fn transfer(runtime, origin, dest: AccountId, value: Balance) { /* implementation */ }
\end{lstlisting}

Then, we use this information to spawn two benchmarks, one with \textbf{connected components} and
one with a \textbf{round robin} distributor. Indeed, we also use a \textbf{sequential} version as
baseline.

The \textbf{dataset} is composed of two parts: the \textbf{initial state} and the
\textbf{transactions}.

The initial state is the state of the world \textit{before} any transactions are executed. In our
case, this maps to a number of initial accounts and an initial balance in each of them. The initial
balance can be parameterized. The second parameter is the number of transactions between the
accounts. Recall that the only transaction in our balances module is \texttt{transfer}. For example,
assuming 100 accounts and 50 transactions, each of the 50 transactions are generated by picking two
random accounts (e.g., Alice and Bob) from the entire set of 100 accounts and creating a
\texttt{transfer(Alice, Bob, Amount)} with a fixed transfer amount.

In this chapter we only focus on a variation of this dataset that we call \textit{the millionaire's
playground}. This is because we assign a very large amount of initial balance to each account,
ensuring that it is many times larger than the transfer amount. Consequently, all transfers will
\textbf{succeed}.

\begin{remark}
	Despite the transfer being a dead-simple transaction, the success or failure branches have
	different \textbf{state access requirements}. Namely, a succeeding transaction accesses the
	balance of both the origin and the destination of the transfer, while a failing one only
	accesses the former. This, next to the value placed as a hint in the access macro can lead to
	interesting combinations that are outside the scope of this chapter, but will be discussed
	further in chapter \ref{chap:conclusion}.
\end{remark}

We assume that there are no time limits imposed on the author of the block, and allow it to finish
executing all transactions. Of course, as we already delineated earlier, this is not how things work
in reality (see \ref{chap_bg:subsec:consensus_authorship}). Nonetheless, this model allows us to be
able to clearly see the throughput difference between the sequential execution and the concurrent
ones.

From within authoring, we do not measure the execution time of the generic distributor. A critical
reader might think this is a way for us to avoid taking the execution time of connected components
into account. We strongly assert otherwise. In most modern blockchains, authors know, well in
advance, if and when they author blocks. Therefore, it is \textit{very} sensible for them run the
the distributor procedure (be it the expensive connected components, or the cheap round robin, or
anything else) over their pool of transactions in advance, as a form of pre-processing. Even if they
do not know when they might author a block, it is rather straightforward to
\textit{periodically/regularly} run the distributor on their local pool\footnote{Lastly, we
anecdotally realized that running a connected components is not a real bottleneck in graphs with
sizes in the order of a few thousand transactions, and with our experimental setup.}. For the
simpler task of validation, we measure the entire process of parsing the block and executing it.

We must also address the state storage issue. In a real-life scenario, the state database is likely
to be kept on high latency storage, and therefore access to new keys might be orders of magnitude
slower (particularly the first one, depending on the caching) than any computation. Our
implementation keeps the entire state in an in-memory \texttt{HashMap}. We acknowledge that this is
likely to be too simplistic and to compensate, we artificially insert \texttt{sleep} operations into
the read and write operations of the final state implementation.

Lastly, we assume that both the authoring and validation uses the same degree of concurrency,
meaning that everyone has the same number of worker threads, ready to receive tasks and
transactions.

\section{Benchmark Results} \label{chap_b&a:sec:results}

For the first demonstration, we fix the number of accounts and gradually create more (transfer)
transactions between them. For all 3 classes of executions (sequential, round robin, connected
components) we generate 1000 members and increase the number of transactions from 250 to 2000. Both
the validation and authoring times are measured. All executions utilize 4 worker threads. Table
\ref{table:bench1} presents all the results in one picture.

\begin{table}[h]
\centering
\caption{Benchmarking results with the millionaire's playground data set. All times are in ms. RR and CC stand from round robin and connected components distributors, respectively. "4" refers to the number of workers.}
\label{table:bench1}
\resizebox{\textwidth}{!} {
	\begin{tabular}{l|rr|rr|rr}
	\textbf{type}    & \textbf{members} & \textbf{transactions} & \textbf{authoring (ms)} & authoring tps   & \textbf{validation (ms)} & validation tps  \\ \hline
	Sequential       & 1000             & 250                   & 1899                    & \textbf{131.65} & 1905                     & \textbf{131.23} \\
	Sequential       & 1000             & 500                   & 3639                    & \textbf{137.40} & 3797                     & \textbf{131.68} \\
	Sequential       & 1000             & 1000                  & 7548                    & \textbf{132.49} & 7508                     & \textbf{133.19} \\
	Sequential       & 1000             & 2000                  & 15053                   & \textbf{132.86} & 14969                    & \textbf{133.61} \\ \hline
	Concurrent(RR-4) & 1000             & 250                   & 898                     & \textbf{278.40} & 704                      & \textbf{355.11} \\
	Concurrent(RR-4) & 1000             & 500                   & 2161                    & \textbf{231.37} & 1789                     & \textbf{279.49} \\
	Concurrent(RR-4) & 1000             & 1000                  & 5517                    & \textbf{181.26} & 4545                     & \textbf{220.02} \\
	Concurrent(RR-4) & 1000             & 2000                  & 12788                   & \textbf{156.40} & 10797                    & \textbf{185.24} \\ \hline
	Concurrent(CC-4) & 1000             & 250                   & 625                     & \textbf{400.00} & 453                      & \textbf{551.88} \\
	Concurrent(CC-4) & 1000             & 500                   & 1210                    & \textbf{413.22} & 887                      & \textbf{563.70} \\
	Concurrent(CC-4) & 1000             & 1000                  & 9510                    & \textbf{105.15} & 7162                     & \textbf{139.63} \\
	Concurrent(CC-4) & 1000             & 2000                  & 19698                   & \textbf{101.53} & 14820                    & \textbf{134.95}
	\end{tabular}
}
\end{table}

Despite our first benchmark being a simple one, it already unravels a great deal of details and
hidden traits about the system's behavior. Thus, we make the following observations:

\subsection*{Sequential}
The sequential execution is nothing special. As expected, the execution time of both tasks
increases linearly as the number of transactions increases. The throughput, as expected, stays
more or less the same.


\subsection*{Round Robin}
The behavior of the round robin distributor degrades over time. With a small number of transactions
(e.g. 250), the throughput increase in authoring is more than 100\%. As the number of transactions
grows more toward 1000, the increase drops. At 1000 transactions, authoring is only a smidgen 20\%
more than the sequential throughput.

The reason for this behavior is of interest. Recall that round robin will distribute the
transactions between the threads with no particular knowledge. This is expected to cause a fairly
large number of transactions to become forwarded or orphans. Our execution logs clearly demonstrate
this behavior. For example, we have the following lines from the execution log of round robin with
250 transactions.

\begin{lstlisting}
[Worker#3] - Sending report Message { payload: AuthoringReport(42, 20), from: 5 }. From 42 executed, 42 were ok and 0 were logic error.
[Worker#1] - Sending report Message { payload: AuthoringReport(45, 18), from: 3 }. From 45 executed, 45 were ok and 0 were logic error.
[Worker#2] - Sending report Message { payload: AuthoringReport(46, 16), from: 4 }. From 46 executed, 46 were ok and 0 were logic error.
[Worker#0] - Sending report Message { payload: AuthoringReport(46, 17), from: 2 }. From 46 executed, 46 were ok and 0 were logic error.
[Master  ] - Finishing Collection phase with [179 executed][32 forwarded][39 orphaned]
\end{lstlisting}

Lines 1-4 contain the \texttt{AuthoringReport} message sent from the worker to master. The two
numeric fields of this message are the number of transactions that got executed and forwarded
respectively. As seen in the log, each worker notified the master that it failed to execute a
portion of their designated transactions. Line 5 is the aggregate information log of the master
thread once all the transactions, except the orphans, are executed. As seen in the log, from the 250
transactions, 179 were executed in their \textit{designated} thread, 32 were forwarded and executed,
and finally 39 were orphaned. This clearly shows the consequence of round robin's \textit{blindness}
toward the \texttt{access} macro. The amount of transactions that got forwarded and orphaned
directly contributes to a reduction in throughput.

Now, we can see the equivalent log in the execution with 1000 transactions.

\begin{lstlisting}
[Master] - Finishing Collection phase with [389 executed][146 forwarded][465 orphaned]
\end{lstlisting}

With 1000 transactions, more than half of them failed to execute in their designated thread, and
ended up being forwarded or orphaned. This further demonstrates why the throughput drops from 250 to
1000 in round robin.

As for validation, we see that the validation throughput is analogous to that of authoring in each
row, but slightly better. Moreover, similar to authoring, the throughput drops as we increase the
transactions. It is very important to understand why. Recall that during validation, the validator
simply uses the tags provided by the author to know which transaction needs to be executed where.
Now, let us analyse the destiny of the imperfect transactions, namely forwarded ones and orphans.
The forwarded transactions will be treated by the validator as if they were not forwarded. These
transactions are still simply assigned to a thread, and the validator can effectively save time by
execution them concurrently, in multiple threads. On the other hand, the orphan transactions are a
\textit{loss} of throughput for \textit{both} the author and the transaction. If a transaction is
declared as orphan, not only the author, but also the validator both lose any throughput gains for
that transaction. This clarifies the throughput trend of the validation in round robin, and how it
drops as we increase the transactions. The underlying reason is, in fact, that as the number of
transactions grows, the number of orphans also increases in the round robin benchmarks. Therefore, a
decline in the throughput of the validator is also expected.

\subsection*{Connected Components}
The outcome of the connected components is even more interesting. For transaction counts 250 and 500
we see much better throughput than both sequential and round robin. This trend applies to both
authoring and validation. Nonetheless, for transaction counts 1000 and 2000 we observe the
throughput plummets down to rates lower than the sequential throughput.

The reason for this can also be explained by examining the logs. First, let us look at the same log
lines for one of the \textit{good} execution, namely 500 transactions.

\begin{lstlisting}
[Worker#2] - Sending report Message { payload: AuthoringReport(125, 0), from: 24 }. From 125 executed, 125 were ok and 0 were logic error.
[Worker#1] - Sending report Message { payload: AuthoringReport(125, 0), from: 23 }. From 125 executed, 125 were ok and 0 were logic error.
[Worker#0] - Sending report Message { payload: AuthoringReport(125, 0), from: 22 }. From 125 executed, 125 were ok and 0 were logic error.
[Worker#3] - Sending report Message { payload: AuthoringReport(125, 0), from: 25 }. From 125 executed, 125 were ok and 0 were logic error.
[Master  ] - Finishing Collection phase with [500 executed][0 forwarded][0 orphaned]
\end{lstlisting}

Interestingly, this time all worker threads executed all of their designated transactions with no
error, leaving no particular leftover work to do for the master thread. Such cases lead to slightly
less than 4-fold throughput gain in authoring and a perfect 4-fold gain in validation.

Now let us examine what goes wrong in case of 1000 transactions.

\begin{lstlisting}
[Worker#3] - Sending report Message { payload: AuthoringReport(8, 0), from: 29 }. From 8 executed, 8 were ok and 0 were logic error.
[Worker#2] - Sending report Message { payload: AuthoringReport(8, 0), from: 28 }. From 8 executed, 8 were ok and 0 were logic error.
[Worker#1] - Sending report Message { payload: AuthoringReport(8, 0), from: 27 }. From 8 executed, 8 were ok and 0 were logic error.
[Worker#0] - Sending report Message { payload: AuthoringReport(976, 0), from: 26 }. From 976 executed, 976 were ok and 0 were logic error.
[Master  ] - Finishing Collection phase with [1000 executed][0 forwarded][0 orphaned]
\end{lstlisting}

In this interesting case, jumping into line 5 of the logs that gives an overview actually unravels
absolutely nothing: It still seems like all the transactions are executed in their designated
thread. The problem in hidden in the number of transactions that each thread received. As seen in
line 1-4, the first 3 threads received a total of 24 transactions, while all the remaining
transactions were given to the last worker thread. The reason for this is rooted in the number of
accounts and transactions. Given 1000 accounts, generating 1000 transfers from them is likely to
create a large blob of \textit{interconnected} transactions. This is something that the connected
components cannot deal well with. The outcome is that a very large chunk of transactions will be
assigned to one giant component, consequently given to one worker thread to execute. This is,
essentially, a typical work imbalance problem that can be seen in different fields of parallel and
concurrent computing. Given this, the connected component execution in this case is basically
sequential, \textit{and} it has a whole lot of overhead from message passing. Therefore, the
throughput drops significantly, even below the sequential one.

As for the validation, we can use the conclusions from the the round robin section to reason about
its behavior. Recall that forwarded transactions are not an overhead for the validator, and only
orphan transactions can cause an overhead. In the case of 1000 and 2000 transactions and connected
components, the validation throughput is almost the same as that of the \textbf{sequential}
execution. This is because the distributor essentially linearizes the transactions into a sequential
group. Moreover, all of the overhead is for the author. Therefore, it is expected that the
throughput of the validator is almost the same as a sequential execution.

The results indicate different traits and characteristics about the entire system as a whole, and
our two distributor components of choice. Round robin is an example of a system in which we use
concurrency but without any pseudo-static hints. To the contrary, connected components is a prime
example of scenarios where the decision of transaction distribution is entirely based upon
pseudo-static hints. While the results are in favour of the latter, we also observed that in some
niche scenarios, connected components is inflexible and therefore turns out to \textit{not} be the
best option. In the next chapter, we summarize these details into the conclusion of our work.
